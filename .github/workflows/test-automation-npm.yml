name: Schengen Calculator Test Automation (npm)

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]
  schedule:
    # Run daily at 2 AM UTC
    - cron: '0 2 * * *'

jobs:
  test-accuracy:
    runs-on: ubuntu-latest
    
    strategy:
      fail-fast: false
      matrix:
        node-version: [18.x, 20.x, 22.x]
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Setup Node.js ${{ matrix.node-version }}
      uses: actions/setup-node@v4
      with:
        node-version: ${{ matrix.node-version }}
        cache: 'npm'
        
    - name: Install dependencies
      run: |
        # Try npm ci first, fallback to npm install if lockfile is out of sync
        if ! npm ci; then
          echo "npm ci failed, trying npm install to regenerate lockfile..."
          npm install
        fi
      
    - name: Run fast tests
      shell: bash
      run: |
        set -o pipefail
        npm run test:fast 2>&1 | tee fast-tests.log
      
    - name: Run full tests
      shell: bash
      run: |
        set -o pipefail
        npm run test:full 2>&1 | tee full-tests.log
      
    - name: Verify log files exist
      run: |
        if [ ! -f fast-tests.log ]; then
          echo "Error: fast-tests.log not found"
          exit 1
        fi
        if [ ! -f full-tests.log ]; then
          echo "Error: full-tests.log not found"
          exit 1
        fi
        echo "Both log files created successfully"
      
    - name: Upload fast test results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: test-results-fast-node-${{ matrix.node-version }}
        path: fast-tests.log
        if-no-files-found: error
        
    - name: Upload full test results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: test-results-full-node-${{ matrix.node-version }}
        path: full-tests.log
        if-no-files-found: error

  eu-compliance-check:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: '20.x'
        cache: 'npm'
        
    - name: Install dependencies
      run: |
        # Try npm ci first, fallback to npm install if lockfile is out of sync
        if ! npm ci; then
          echo "npm ci failed, trying npm install to regenerate lockfile..."
          npm install
        fi
      
    - name: Run EU compliance validation
      shell: bash
      run: |
        set -o pipefail
        npm run test:eu-compliance 2>&1 | tee eu-compliance-report.txt
      
    - name: Verify compliance report exists
      run: |
        if [ ! -f eu-compliance-report.txt ]; then
          echo "Error: eu-compliance-report.txt not found"
          exit 1
        fi
        echo "EU compliance report file created successfully"
        
    - name: Check EU compliance status
      run: |
        if grep -q "FAIL" eu-compliance-report.txt; then
          echo "EU compliance tests failed"
          exit 1
        else
          echo "EU compliance tests passed"
        fi
        
    - name: Upload EU compliance report
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: eu-compliance-report
        path: eu-compliance-report.txt
        if-no-files-found: error

  edge-case-validation:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: '20.x'
        cache: 'npm'
        
    - name: Install dependencies
      run: |
        # Try npm ci first, fallback to npm install if lockfile is out of sync
        if ! npm ci; then
          echo "npm ci failed, trying npm install to regenerate lockfile..."
          npm install
        fi
      
    - name: Run edge case tests
      shell: bash
      run: |
        set -o pipefail
        # Run edge case tests and save logs
        npm run test:date-overlap 2>&1 | tee edge-case-report.txt
        
    - name: Check edge case status
      run: |
        if grep -q "FAIL" edge-case-report.txt; then
          echo "Edge case tests failed"
          exit 1
        else
          echo "Edge case tests passed"
        fi
        
    - name: Upload edge case report
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: edge-case-report
        path: edge-case-report.txt
        if-no-files-found: error

  performance-benchmark:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: '20.x'
        cache: 'npm'
        
    - name: Install dependencies
      run: |
        # Try npm ci first, fallback to npm install if lockfile is out of sync
        if ! npm ci; then
          echo "npm ci failed, trying npm install to regenerate lockfile..."
          npm install
        fi
      
    - name: Run performance benchmarks
      shell: bash
      run: |
        set -o pipefail
        npm run benchmark 2>&1 | tee performance-log.txt
      
    - name: Verify performance log exists
      run: |
        if [ ! -f performance-log.txt ]; then
          echo "Error: performance-log.txt not found"
          exit 1
        fi
        echo "Performance log file created successfully"
        
    - name: Analyze performance results
      run: |
        echo "Performance Analysis:"
        cat performance-log.txt
        echo ""
        grep -E "(Performance:|ms|PASS|FAIL)" performance-log.txt || echo "No performance data found"
        
    - name: Upload performance results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: performance-results
        path: performance-log.txt
        if-no-files-found: error

  test-summary:
    runs-on: ubuntu-latest
    needs: [test-accuracy, eu-compliance-check, edge-case-validation, performance-benchmark]
    if: always()
    
    steps:
    - name: Download all artifacts
      uses: actions/download-artifact@v4
      
    - name: Debug - List downloaded artifacts
      run: |
        echo "=== Current directory structure ==="
        pwd
        ls -la
        echo ""
        echo "=== All downloaded artifacts ==="
        find . -type f -name "*.log" -o -name "*.txt" 2>/dev/null || echo "No log/txt files found"
        echo ""
        echo "=== Directory tree ==="
        ls -R || echo "Failed to list directory tree"
      
    - name: Generate test summary
      shell: bash
      run: |
        set -e  # Exit on error
        set -o pipefail  # Fail on pipe errors
        
        # Debug information
        echo "=== Starting test summary generation ==="
        echo "Current directory: $(pwd)"
        echo "Shell: $SHELL"
        echo "User: $(whoami)"
        echo ""
        
        # Create the summary file with error handling
        echo "Creating test-summary.md file..."
        echo "# 🧪 Schengen Calculator Test Summary" > test-summary.md || {
          echo "ERROR: Failed to create test-summary.md"
          echo "Checking permissions..."
          ls -la .
          echo "Trying with explicit path..."
          touch ./test-summary.md || echo "Touch failed"
          exit 1
        }
        echo "" >> test-summary.md
        echo "**Generated:** $(date -u)" >> test-summary.md
        echo "**Workflow:** ${{ github.workflow }}" >> test-summary.md
        echo "**Run:** ${{ github.run_number }}" >> test-summary.md
        echo "" >> test-summary.md
        echo "## 📊 Test Results Overview" >> test-summary.md
        echo "" >> test-summary.md
        
        # Initialize counters (use declare for bash arithmetic)
        declare -i TOTAL_TESTS=0
        declare -i PASSED_TESTS=0
        declare -i FAILED_TESTS=0
        
        # Check accuracy tests across all Node.js versions
        echo "### 🎯 Accuracy Tests" >> test-summary.md
        for version in 18.x 20.x 22.x; do
          version_clean=$(echo $version | tr '.' '-')
          
          # Check for fast test results
          fast_dir="test-results-fast-node-${version}"
          full_dir="test-results-full-node-${version}"
          
          echo "Checking for artifacts in ${fast_dir} and ${full_dir}..." >&2
          
          if [ -f "${fast_dir}/fast-tests.log" ] || [ -f "${full_dir}/full-tests.log" ]; then
            test_passed=true
            
            # Check fast tests
            if [ -f "${fast_dir}/fast-tests.log" ]; then
              if ! grep -q "✅ PASS\|All tests passed" "${fast_dir}/fast-tests.log"; then
                test_passed=false
              fi
            fi
            
            # Check full tests
            if [ -f "${full_dir}/full-tests.log" ]; then
              if ! grep -q "✅ PASS\|All tests passed" "${full_dir}/full-tests.log"; then
                test_passed=false
              fi
            fi
            
            if [ "$test_passed" = true ]; then
              echo "- ✅ Node.js ${version}: PASSED" >> test-summary.md
              PASSED_TESTS=$((PASSED_TESTS + 1))
            else
              echo "- ❌ Node.js ${version}: FAILED" >> test-summary.md
              FAILED_TESTS=$((FAILED_TESTS + 1))
            fi
            TOTAL_TESTS=$((TOTAL_TESTS + 1))
          else
            echo "- ⚠️ Node.js ${version}: NO RESULTS" >> test-summary.md
            FAILED_TESTS=$((FAILED_TESTS + 1))
            TOTAL_TESTS=$((TOTAL_TESTS + 1))
          fi
        done
        echo "" >> test-summary.md
        
        # Check EU compliance
        echo "### 🇪🇺 EU Compliance" >> test-summary.md
        if [ -f eu-compliance-report/eu-compliance-report.txt ]; then
          if grep -q "✅" eu-compliance-report/eu-compliance-report.txt; then
            echo "- ✅ EU Official Test Cases: PASSED" >> test-summary.md
            PASSED_TESTS=$((PASSED_TESTS + 1))
          else
            echo "- ❌ EU Official Test Cases: FAILED" >> test-summary.md
            FAILED_TESTS=$((FAILED_TESTS + 1))
          fi
          TOTAL_TESTS=$((TOTAL_TESTS + 1))
        else
          echo "- ⚠️ EU Official Test Cases: NO RESULTS" >> test-summary.md
          FAILED_TESTS=$((FAILED_TESTS + 1))
          TOTAL_TESTS=$((TOTAL_TESTS + 1))
        fi
        echo "" >> test-summary.md
        
        # Check edge cases
        echo "### 🔧 Edge Case Validation" >> test-summary.md
        if [ -f edge-case-report/edge-case-report.txt ]; then
          if grep -q "✅" edge-case-report/edge-case-report.txt; then
            echo "- ✅ Edge Case Handling: PASSED" >> test-summary.md
            PASSED_TESTS=$((PASSED_TESTS + 1))
          else
            echo "- ❌ Edge Case Handling: FAILED" >> test-summary.md
            FAILED_TESTS=$((FAILED_TESTS + 1))
          fi
          TOTAL_TESTS=$((TOTAL_TESTS + 1))
        else
          echo "- ⚠️ Edge Case Handling: NO RESULTS" >> test-summary.md
          FAILED_TESTS=$((FAILED_TESTS + 1))
          TOTAL_TESTS=$((TOTAL_TESTS + 1))
        fi
        echo "" >> test-summary.md
        
        # Check performance
        echo "### ⚡ Performance Benchmarks" >> test-summary.md
        if [ -f performance-results/performance-log.txt ]; then
          if grep -q "✅" performance-results/performance-log.txt; then
            echo "- ✅ Performance Tests: PASSED" >> test-summary.md
            PASSED_TESTS=$((PASSED_TESTS + 1))
          else
            echo "- ⚠️ Performance Tests: CHECK REQUIRED" >> test-summary.md
            FAILED_TESTS=$((FAILED_TESTS + 1))
          fi
          TOTAL_TESTS=$((TOTAL_TESTS + 1))
        else
          echo "- ⚠️ Performance Tests: NO RESULTS" >> test-summary.md
          FAILED_TESTS=$((FAILED_TESTS + 1))
          TOTAL_TESTS=$((TOTAL_TESTS + 1))
        fi
        echo "" >> test-summary.md
        
        # Overall summary
        echo "## 📈 Summary" >> test-summary.md
        echo "" >> test-summary.md
        echo "| Metric | Value |" >> test-summary.md
        echo "|--------|-------|" >> test-summary.md
        echo "| Total Tests | ${TOTAL_TESTS} |" >> test-summary.md
        echo "| Passed | ${PASSED_TESTS} |" >> test-summary.md
        echo "| Failed | ${FAILED_TESTS} |" >> test-summary.md
        
        if [ $TOTAL_TESTS -gt 0 ]; then
          PASS_RATE=$((PASSED_TESTS * 100 / TOTAL_TESTS))
          echo "| Pass Rate | ${PASS_RATE}% |" >> test-summary.md
        else
          echo "| Pass Rate | N/A |" >> test-summary.md
        fi
        
        echo "" >> test-summary.md
        
        # Final status
        if [ $FAILED_TESTS -eq 0 ] && [ $TOTAL_TESTS -gt 0 ]; then
          echo "🎉 **All tests passed successfully!**" >> test-summary.md
        elif [ $TOTAL_TESTS -eq 0 ]; then
          echo "⚠️ **No test results found - check workflow execution**" >> test-summary.md
        else
          echo "❌ **Some tests failed - review results above**" >> test-summary.md
        fi
        
        echo "" >> test-summary.md
        echo "---" >> test-summary.md
        echo "*Generated by GitHub Actions on $(date -u)*" >> test-summary.md
        
        # Validate the file was created
        echo ""
        echo "=== Validating test-summary.md ==="
        if [ ! -f test-summary.md ]; then
          echo "ERROR: test-summary.md was not created!"
          exit 1
        fi
        
        file_size=$(stat -c%s test-summary.md 2>/dev/null || stat -f%z test-summary.md 2>/dev/null || echo "0")
        echo "File size: ${file_size} bytes"
        
        if [ "$file_size" -eq "0" ]; then
          echo "ERROR: test-summary.md is empty!"
          exit 1
        fi
        
        echo "SUCCESS: test-summary.md created successfully"
        echo ""
        echo "=== Test Summary Content ==="
        cat test-summary.md
        
    - name: Validate summary file before upload
      run: |
        if [ ! -f test-summary.md ]; then
          echo "ERROR: test-summary.md does not exist!"
          ls -la
          exit 1
        fi
        echo "test-summary.md exists and is ready for upload"
        echo "File info:"
        ls -la test-summary.md
        
    - name: Upload test summary
      uses: actions/upload-artifact@v4
      with:
        name: test-summary
        path: test-summary.md
        if-no-files-found: error